# Ротова Юлия МКИС33
Отчет 3

Разработка датасета с точки зрения Data Scientist
 
Моя задача заключалась в анализе данных, извлечении полезных инсайтов и подготовке датасета для анализа с использованием методов статистики, визуализации данных и машинного обучения. Важно было не только подготовить данные, но и понять, какие из них будут полезны для решения бизнес-задач, таких как анализ предпочтений пользователей и прогнозирование будущих тенденций в кинематографе.
Задачи и цели
Целью было создать полезный и легко анализируемый датасет, который поможет извлечь полезные инсайты из текстов описаний фильмов. Мы планировали использовать данные для следующих целей:
- Анализ содержания описаний фильмов: Изучение ключевых тем и мотивов, которые часто встречаются в фильмах.
- Выявление закономерностей: Например, исследование взаимосвязи между жанром фильма и его описанием, а также анализ того, какие типы фильмов наиболее популярны среди пользователей.
- Модели предсказания: Создание модели для предсказания, какому типу зрителей могут быть интересны различные фильмы, на основе их описаний.
Этапы разработки 
1. Изучение и анализ исходных данных:
   Первоначально я провел анализ содержания датасета, чтобы понять, какие данные доступны для анализа. Важнейшими элементами были:
   - Идентификатор фильма – помогает отслеживать уникальные фильмы.
   - Название фильма – помогает быстро понять, о каком фильме идет речь.
   - Описание фильма – основной источник информации для анализа. Описание фильма дает нам информацию о сюжете, персонажах и других важных аспектах.
 
2. Предобработка данных:
   После изучения данных я приступил к их предварительной обработке:
   - Очистка текста: Удаление лишних символов, исправление опечаток и приведение текста к единому регистру.
   - Токенизация: Разбиение текста на отдельные слова, чтобы было легче анализировать ключевые темы в каждом фильме.
   - Удаление стоп-слов: Это помогло уменьшить количество малозначимых слов, которые не оказывают влияния на анализ.
   - Лемматизация: Приведение слов к их начальной форме, чтобы минимизировать количество уникальных токенов.
 
3. Анализ данных:
   Когда данные были очищены, я начал проводить анализ:
   - Частотный анализ: Выявление самых часто встречающихся слов и фраз в описаниях фильмов.
   - Тематический анализ: Использование методов, таких как LDA (Latent Dirichlet Allocation), для выделения ключевых тем в описаниях фильмов.
   - Корреляционный анализ: Исследование взаимосвязи между жанрами фильмов и ключевыми словами в их описаниях.
 
4. Визуализация данных:
   Для более глубокого понимания закономерностей я создал несколько визуализаций:
   - Гистограммы и графики распределения: Например, распределение по годам выпуска фильмов, распределение жанров.
   - Тематические карты: Для визуализации, какие темы встречаются в различных жанрах.
   - Тепловые карты: Для отображения корреляции между описаниями фильмов и их рейтингами или жанрами.
 
5. Модели предсказания и кластеризация:
   Важной частью работы было создание модели для предсказания жанра фильма по его описанию. Для этого я применил различные методы:
   - Классификация текста с использованием алгоритмов, таких как Logistic Regression и Random Forest.
   - Кластеризация фильмов на основе описаний с помощью алгоритма K-Means, чтобы группировать фильмы с похожими сюжетами.
 Инструменты и библиотеки
Для анализа и обработки данных я использовал следующие инструменты и библиотеки:
- Pandas и NumPy – для работы с данными и проведения базового статистического анализа.
- NLTK и spaCy – для предобработки текста (токенизация, лемматизация).
- Matplotlib и Seaborn – для визуализации данных.
- scikit-learn – для построения моделей машинного обучения и классификации текстов.
- Gensim – для тематического моделирования и анализа ключевых тем.

